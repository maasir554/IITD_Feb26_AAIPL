{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a0ec7783-45f0-459f-af55-ab8043a11d4d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exists: True\n",
      "Files: ['model-00008-of-00008.safetensors', 'model-00006-of-00008.safetensors', 'generation_config.json', 'model-00005-of-00008.safetensors', 'README.md', 'config.json', '.ipynb_checkpoints', 'model.safetensors.index.json', 'vocab.json', 'model-00003-of-00008.safetensors', 'LICENSE', 'merges.txt', 'model-00001-of-00008.safetensors', 'model-00007-of-00008.safetensors', 'model-00002-of-00008.safetensors', 'tokenizer.json', 'tokenizer_config.json', '.gitattributes', 'model-00004-of-00008.safetensors']\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import time\n",
    "\n",
    "path = \"/workspace/AAIPL/hf_models/Qwen2.5-14B-Instruct\"\n",
    "\n",
    "print(\"Exists:\", os.path.exists(path))\n",
    "print(\"Files:\", os.listdir(path))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c419ab72-a437-4e11-8e01-047a7837745f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ü¶• Unsloth: Will patch your computer to enable 2x faster free finetuning.\n",
      "#### Unsloth: `hf_xet==1.1.10` and `ipykernel>6.30.1` breaks progress bars. Disabling for now in XET.\n",
      "#### Unsloth: To re-enable progress bars, please downgrade to `ipykernel==6.30.1` or wait for a fix to\n",
      "https://github.com/huggingface/xet-core/issues/526\n",
      "INFO 02-15 05:02:52 [__init__.py:225] Automatically detected platform rocm.\n",
      "ü¶• Unsloth Zoo will now patch everything to make training faster!\n",
      "Unsloth: AMD currently is not stable with 4bit bitsandbytes. Disabling for now.\n",
      "==((====))==  Unsloth 2025.10.9: Fast Qwen2 patching. Transformers: 4.56.2. vLLM: 0.11.1rc3.dev39+gf417746ad.rocm700.\n",
      "   \\\\   /|    . Num GPUs = 1. Max memory: 255.688 GB. Platform: Linux.\n",
      "O^O/ \\_/ \\    Torch: 2.9.0a0+git1c57644. ROCm Toolkit: 7.0.51831-a3e329ad8. Triton: 3.4.0\n",
      "\\        /    Bfloat16 = TRUE. FA [Xformers = None. FA2 = True]\n",
      " \"-____-\"     Free license: http://github.com/unslothai/unsloth\n",
      "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "73c9f33d6d3147ea8867ca590474495f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/8 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unsloth 2025.10.9 patched 48 layers with 48 QKV layers, 48 O layers and 48 MLP layers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded successfully ‚úÖ\n"
     ]
    }
   ],
   "source": [
    "from unsloth import FastLanguageModel\n",
    "import torch\n",
    "\n",
    "model_path = \"/workspace/AAIPL/outputs/final\"\n",
    "\n",
    "model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "    model_name = model_path,\n",
    "    max_seq_length = 1024,          # safer first load\n",
    ")\n",
    "\n",
    "FastLanguageModel.for_inference(model)\n",
    "\n",
    "print(\"Loaded successfully ‚úÖ\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "6d9ffd0b-166b-43f1-9ad3-3d0a92abec78",
   "metadata": {},
   "outputs": [],
   "source": [
    "messages = [\n",
    "    {\n",
    "        \"role\": \"system\",\n",
    "        \"content\": \"\"\"\n",
    "You are an expert logical reasoning generator.\n",
    "\n",
    "Rules:\n",
    "- Output valid JSON only\n",
    "- Generate EXACTLY one question\n",
    "- Difficulty MUST be VERY HARD\n",
    "- Do NOT fabricate complexity\n",
    "\"\"\"\n",
    "    },\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": f\"\"\"\n",
    "Generate exactly ONE VERY HARD logical reasoning question.\n",
    "\n",
    "Topic: Alphanumeric Series\n",
    "\n",
    "Difficulty constraints:\n",
    "- Minimum 6 terms\n",
    "- Include alternating pattern logic\n",
    "- Include nonlinear number changes\n",
    "- Include irregular letter shifts\n",
    "- Must require multi-step reasoning\n",
    "- Must NOT be solvable by simple arithmetic progression\n",
    "- Explanation must describe the TRUE underlying pattern\n",
    "\n",
    "JSON format:\n",
    "{{\n",
    "    \"topic\": \"Alphanumeric Series\",\n",
    "    \"question\": \"<full question>\",\n",
    "    \"choices\": [\n",
    "        \"A) ...\",\n",
    "        \"B) ...\",\n",
    "        \"C) ...\",\n",
    "        \"D) ...\"\n",
    "    ],\n",
    "    \"answer\": \"<letter>\",\n",
    "    \"explanation\": \"<‚â§100 words>\"\n",
    "}}\n",
    "\"\"\"\n",
    "    }\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "33b51ecc-93fe-4b4f-a15a-82c44455de4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = tokenizer.apply_chat_template(\n",
    "    messages,\n",
    "    tokenize=True,\n",
    "    add_generation_prompt=True,\n",
    "    return_tensors=\"pt\"\n",
    ").to(\"cuda\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "86b4889d-eb8f-4d7e-ae25-0d0e5fada6d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = time.time()\n",
    "\n",
    "# outputs = model.generate(\n",
    "#     **inputs,\n",
    "#     max_new_tokens=500,\n",
    "#     temperature=0.7,\n",
    "#     top_p=0.9,\n",
    "#     do_sample=True,\n",
    "# )\n",
    "\n",
    "outputs = model.generate(\n",
    "    input_ids = inputs,\n",
    "    max_new_tokens = 500,\n",
    "    temperature = 0.7,\n",
    "    top_p = 0.9,\n",
    "    do_sample = True,\n",
    ")\n",
    "\n",
    "\n",
    "end_time = time.time()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "c381e4c3-5206-481f-bff0-519974c9a5c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated Output:\n",
      "\n",
      "{\n",
      "    \"topic\": \"Alphanumeric Series\",\n",
      "    \"question\": \"Identify the next term in the series: B2, D5, G9, K14, O20, ?\",\n",
      "    \"choices\": [\n",
      "        \"A) R27\",\n",
      "        \"B) S28\",\n",
      "        \"C) T30\",\n",
      "        \"D) U32\"\n",
      "    ],\n",
      "    \"answer\": \"D\",\n",
      "    \"explanation\": \"The series alternates between letter and number patterns. Letters advance by increasing intervals (1, 2, 3, 4), while numbers follow a quadratic increment (2, 5, 9, 14, 20). The next letter is U (+5 from O) and the next number is 32 (quadratic increment from 20).\"\n",
      "}\n",
      "\n",
      "‚è± Time taken: 3.818 seconds\n"
     ]
    }
   ],
   "source": [
    "response = tokenizer.decode(\n",
    "    outputs[0][inputs.shape[1]:],   # ‚úÖ slice ‡§Ü‡§â‡§ü input ‡§π‡§ü‡§æ‡§ì\n",
    "    skip_special_tokens=True\n",
    ")\n",
    "\n",
    "print(\"Generated Output:\\n\")\n",
    "print(response)\n",
    "\n",
    "print(\"\\n‚è± Time taken:\", round(end_time - start_time, 3), \"seconds\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33e39d0b-b438-47db-b5fa-2bc31cf900e1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

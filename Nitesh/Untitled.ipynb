{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5ed13729-f585-4f0b-962d-cf5bd0167877",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = \"/workspace/AAIPL/hf_models/Qwen2.5-14B-Instruct/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6b88e9d0-2dc4-4cf1-8e37-7af0dc5111fb",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unsloth: AMD currently is not stable with 4bit bitsandbytes. Disabling for now.\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Unsloth: No config file found - are you sure the `model_name` is correct?\nIf you're using a model on your local device, confirm if the folder location exists.\nIf you're using a HuggingFace online model, check if it exists.",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mRuntimeError\u001b[39m                              Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[9]\u001b[39m\u001b[32m, line 5\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01munsloth\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m FastLanguageModel\n\u001b[32m      2\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorch\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m5\u001b[39m model, tokenizer = \u001b[43mFastLanguageModel\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m      6\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmodel_name\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      7\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmax_seq_length\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m4096\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m     \u001b[49m\u001b[38;5;66;43;03m# adjust if needed\u001b[39;49;00m\n\u001b[32m      8\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m              \u001b[49m\u001b[38;5;66;43;03m# auto detect (fp16/bf16)\u001b[39;49;00m\n\u001b[32m      9\u001b[39m \u001b[43m    \u001b[49m\u001b[43mload_in_4bit\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m       \u001b[49m\u001b[38;5;66;43;03m# âœ… recommended for 14B\u001b[39;49;00m\n\u001b[32m     10\u001b[39m \u001b[43m)\u001b[49m\n\u001b[32m     12\u001b[39m FastLanguageModel.for_inference(model)  \u001b[38;5;66;03m# ðŸš€ optimizes inference\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.12/dist-packages/unsloth/models/loader.py:281\u001b[39m, in \u001b[36mFastLanguageModel.from_pretrained\u001b[39m\u001b[34m(model_name, max_seq_length, dtype, load_in_4bit, load_in_8bit, load_in_16bit, full_finetuning, token, device_map, rope_scaling, fix_tokenizer, trust_remote_code, use_gradient_checkpointing, resize_model_vocab, revision, use_exact_model_name, offload_embedding, fast_inference, gpu_memory_utilization, float8_kv_cache, random_state, max_lora_rank, disable_log_stats, qat_scheme, *args, **kwargs)\u001b[39m\n\u001b[32m    274\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m both_exist:\n\u001b[32m    275\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[32m    276\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mUnsloth: Your repo has a LoRA adapter and a base model.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m\\\n\u001b[32m    277\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mYou have 2 files `config.json` and `adapter_config.json`.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m\\\n\u001b[32m    278\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mWe must only allow one config file.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m\\\n\u001b[32m    279\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mPlease separate the LoRA and base models to 2 repos.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    280\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m281\u001b[39m model_types = \u001b[43mget_transformers_model_type\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    282\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpeft_config\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mpeft_config\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mis\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mmodel_config\u001b[49m\n\u001b[32m    283\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    284\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(model_types) == \u001b[32m1\u001b[39m:\n\u001b[32m    285\u001b[39m     model_type = model_types[\u001b[32m0\u001b[39m]\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.12/dist-packages/unsloth_zoo/hf_utils.py:112\u001b[39m, in \u001b[36mget_transformers_model_type\u001b[39m\u001b[34m(config)\u001b[39m\n\u001b[32m    110\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\" Gets model_type from config file - can be PEFT or normal HF \"\"\"\u001b[39;00m\n\u001b[32m    111\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m config \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m112\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[32m    113\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mUnsloth: No config file found - are you sure the `model_name` is correct?\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m\\\n\u001b[32m    114\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mIf you\u001b[39m\u001b[33m'\u001b[39m\u001b[33mre using a model on your local device, confirm if the folder location exists.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m\\\n\u001b[32m    115\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mIf you\u001b[39m\u001b[33m'\u001b[39m\u001b[33mre using a HuggingFace online model, check if it exists.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    116\u001b[39m     )\n\u001b[32m    117\u001b[39m model_types = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    119\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpeft\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m PeftConfig\n",
      "\u001b[31mRuntimeError\u001b[39m: Unsloth: No config file found - are you sure the `model_name` is correct?\nIf you're using a model on your local device, confirm if the folder location exists.\nIf you're using a HuggingFace online model, check if it exists."
     ]
    }
   ],
   "source": [
    "from unsloth import FastLanguageModel\n",
    "import torch\n",
    "\n",
    "\n",
    "model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "    model_name = model_path,\n",
    "    max_seq_length = 4096,     # adjust if needed\n",
    "    dtype = None,              # auto detect (fp16/bf16)\n",
    "    load_in_4bit = False,       # âœ… recommended for 14B\n",
    ")\n",
    "\n",
    "FastLanguageModel.for_inference(model)  # ðŸš€ optimizes inference\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2c0e3739-1bc0-4fe8-9560-dd57caffe12e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ls: cannot access 'AAIPL/hf_models/Qwen2.5-14B-Instruct': No such file or directory\n"
     ]
    }
   ],
   "source": [
    "!ls AAIPL/hf_models/Qwen2.5-14B-Instruct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abed8cb9-66fe-49b9-8617-8f0ede28ad46",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
